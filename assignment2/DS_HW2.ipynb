{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.feature_selection import RFE\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Boston1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Download the \"Boston1.csv\" database, and explore the data. Explanation about the dataset can be found here: http://www.clemson.edu/economics/faculty/wilson/R-tutorial/analyzing_data.html\n",
    "\n",
    "Find the columns with missing values and filter them out of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(\"misData\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Divide the filtered data randomly into a train set (70% of the data) and test set (30% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done this previously, install the scikit-learn package for python.\n",
    "\n",
    "a) On the train set, run a linear regression model as follows:\n",
    "Divide the training set into explanatory variables (the X matrix with which we'll try to make a prediction) and a target variable (y, the value which we'll try to predict). Use the 'medv' attribute as the target variable y and the rest of the features as the X matrix. Run a linear regression model on those sets, and print the regression coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.00219941e-01   4.16282089e-02   4.72872037e-03   8.05460632e-01\n",
      "  -1.38349871e+01   4.24091837e+00  -4.31670987e-03  -1.34477791e+00\n",
      "   2.92306610e-01  -1.45328669e-02  -9.19841387e-01   1.14779107e-02\n",
      "  -4.61226223e-01  -1.87642314e-01]\n"
     ]
    }
   ],
   "source": [
    "def create_linear_model(df, y_cols, print_mse=False):\n",
    "    train_y = df[y_cols]\n",
    "    train_X = df.drop(y_cols, axis=1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X, train_y)\n",
    "    if print_mse:\n",
    "        print(mse(model.predict(train_X), train_y))\n",
    "    return model, train_X, train_y\n",
    "\n",
    "model, train_X, train_y = create_linear_model(train_df, \"medv\")\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the linear regression model to predict the values of the test set's 'medv' column, based on the test set's other attributes. Print the Mean Squared Error of the model on the train set and on the test set.\n",
    "Usually, the MSE on the train set would be lower than the MSE on the test set, since the model parameters are optimized with respect to the train set. Must this always be the case? Can you think of a few examples for when this might not be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6158934983\n",
      "28.8485792288\n"
     ]
    }
   ],
   "source": [
    "test_y = test_df.medv\n",
    "test_X = test_df.drop(\"medv\", axis=1)\n",
    "print(mse(model.predict(train_X), train_y))\n",
    "print(mse(model.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Add some noise (with mean=0, std=1) to the test set's y, and predict it again. What happened to the MSE? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6158934983\n",
      "31.7702645464\n"
     ]
    }
   ],
   "source": [
    "noised_test_y = test_y + np.random.normal(0, 1)\n",
    "print(mse(model.predict(train_X), train_y))\n",
    "print(mse(model.predict(test_X), noised_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create a Recursive feature elimination model, with a linear regression estimator, that selects half of the original number of features. Hint: Check the feature_selection module in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features_to_select = train_X.shape[1] // 2\n",
    "rfe = RFE(estimator=model, n_features_to_select=n_features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the feature elimination model on the full database (after filtering columns with missing values, before partitioning into train/test). Print the features that were selected. Remember that we separate the 'medv' attribute to be our y, while the rest of the attributes in the dataset serve as features to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['chas', 'nox', 'rm', 'dis', 'ptratio', 'lstat', 'randCol'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(\"medv\", axis=1)\n",
    "y = df.medv\n",
    "rfe.fit(X, y)\n",
    "print(df.columns.drop(\"medv\")[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We'd like to find out the optimal number of features. Create feature elimination models (with linear regression estimators) for every number of features between 1 and n (where n = all the original features, 'medv' excluded). For each number of features, run a linear regression as in Question 2, only on the selected features, in order to predict 'medv'. Print the Mean Sqaured Error for each number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.0042883554\n",
      "39.2181167428\n",
      "37.5182698877\n",
      "32.4469471815\n",
      "30.9295460795\n",
      "23.9942148931\n",
      "23.988679676\n",
      "23.8731401756\n",
      "23.3449296742\n",
      "23.2555604454\n",
      "22.9303616104\n",
      "22.4250286244\n",
      "21.8808608279\n",
      "21.8807216167\n"
     ]
    }
   ],
   "source": [
    "def get_df_after_feature_selection(origin_df, base_model, number_of_features_to_select, y_cols):\n",
    "    rfe = RFE(estimator=base_model, n_features_to_select=number_of_features_to_select)\n",
    "    X = df.drop(y_cols, axis=1)\n",
    "    y = df[y_cols]\n",
    "    rfe.fit(X, y)\n",
    "    return origin_df[np.append(df.columns.drop(y_cols)[rfe.support_], y_cols)]\n",
    "    \n",
    "y_cols = \"medv\"\n",
    "for i in range(1, X.shape[1] + 1):\n",
    "    new_df = get_df_after_feature_selection(df, model, i, y_cols)\n",
    "    model_i, train_X, train_y = create_linear_model(new_df, y_cols, print_mse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Conclude the optimal number of features for this task. Think about the cost of adding for data vs the benefit of a more accurate prediction. Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.9942148931\n"
     ]
    }
   ],
   "source": [
    "new_df = get_df_after_feature_selection(df, model, 6, y_cols)\n",
    "six_features_model, X, y = create_linear_model(new_df, y_cols, print_mse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a cross-validation of the linear regression on the train set with K=5. Print the CV scores for each repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64663051,  0.73709447,  0.56867765,  0.17489251,  0.13451655])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold = 5\n",
    "cross_val_score(six_features_model, X, y, cv=k_fold)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
